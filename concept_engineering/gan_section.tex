Generative Adversarial Networks (GANs) are deep learning models that consist of two neural networks.
The first is a generator \( G \), which is responsible for generating false samples from random noise, and the second is a discriminator \( D \), which tries to detect which samples are false.
The generator and the discriminator compete in a minimax game. The two networks are trained simultaneously with opposite goals.

Mathematically, the goal of GANs is represented by the following minimax objective function:

\begin{equation}
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log (1 - D(G(z)))]
\end{equation}
where:

\begin{itemize}
\item \( D(x) \) represents the probability that the discriminator correctly identifies a real sample \( x \),

\item \( G(z)=\hat{x} \) represents the synthetic data generated by the generator from Gaussian noise \( z \) $\sim\mathcal{N}(0,1)$, in our case CT scan,

\item \( p_{data}(x) \) is the data distribution of the real samples,

\item \( p_z(z) \) is the distribution of the random noise, generator's input.
\end{itemize}

\paragraph{Loss function}\mbox{}\\
\indent GAN loss function can be calculated as
\begin{equation}
    L_{GAN}(N,D) = [\log D(x) + log(1-D(\hat{x}))].
\label{loss_gan}
\end{equation}
However it often leads to discriminator being too good on the beginning and in result generator stops learning. To mitigate that, each neural network, generator and discriminator has their own loss.
The loss function of the discriminator is typically defined as

\begin{equation}
L_D = - \left( \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log (1 - D(\hat{x}))] \right).
\end{equation}


This loss encourages the discriminator to maximize its ability to distinguish real from false data. On the other hand, the generator loss function can be defined as

\begin{equation}
L_G = - \mathbb{E}_{z \sim p_z(z)}[\log D(\hat{x})].
\end{equation}


This loss encourages the generator to improve the quality of the false samples by "fooling" the discriminator into classifying them as real. Training continues until the generator produces samples that are indistinguishable from the real data, at which point the discriminator cannot distinguish between real and generated samples and a Nash equilibrium\footnote{\url{https://en.wikipedia.org/wiki/Nash_equilibrium}} is reached.

GANs have had a significant impact in areas such as image generation, data augmentation and even super-resolution tasks, but their training can be unstable due to problems such as vanishing gradients. 


\paragraph{Generation of CT scan}\mbox{}\\

To generate an artifical CT scan using GAN, one should sample a Gaussian noise tensor $z$ of the same shape as the input of the generator from $\mathcal{N}(0,1)$. Then pass it to the generator, which would produce a synthetic CT scan as a result.