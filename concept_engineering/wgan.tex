% \input{gan_section}
% ---

% **References:**
% 1. Goodfellow, I. et al. (2014). *Generative Adversarial Nets*. Advances in Neural Information Processing Systems.
% 2. Arjovsky, M., Chintala, S., & Bottou, L. (2017). *Wasserstein GAN*.
% \paragraph{WGAN}
% Wasserstein Generative Adversarial Networks (WGANs) are an improvement over traditional GANs, addressing issues like training instability and mode collapse by using the Wasserstein distance (also known as Earth Mover's Distance) as a metric for comparing the real and generated data distributions. Introduced by Arjovsky et al. (2017), WGAN replaces the original GAN loss function with one that provides better gradient behavior, improving training stability and convergence.

% In a WGAN, the discriminator is renamed as the "critic" because instead of classifying real vs. fake data, it evaluates the Wasserstein distance between the real and generated data distributions. The Wasserstein distance \( W(p_{data}, p_G) \) between the real data distribution \( p_{data} \) and the generator distribution \( p_G \) is defined as:

% \[
% W(p_{data}, p_G) = \inf_{\gamma \in \Pi(p_{data}, p_G)} \mathbb{E}_{(x,y) \sim \gamma} [||x - y||]
% \]

% where \( \Pi(p_{data}, p_G) \) represents all possible joint distributions \( \gamma(x, y) \) whose marginals are \( p_{data} \) and \( p_G \). In simpler terms, it measures the minimum cost of transporting the mass from one distribution to another.

% ### WGAN Loss Function

% Unlike traditional GANs, which use a sigmoid output for the discriminator and binary cross-entropy loss, WGAN uses the following loss functions without any activation at the output of the critic. The critic's loss function is:

% \[
% L_C = \mathbb{E}_{x \sim p_{data}(x)}[D(x)] - \mathbb{E}_{z \sim p_z(z)}[D(G(z))]
% \]

% The critic aims to maximize the difference between the critic's output for real samples \( D(x) \) and generated samples \( D(G(z)) \), which corresponds to minimizing the Wasserstein distance.

% The generator's loss function, on the other hand, is designed to minimize the critic's output for generated samples:

% \[
% L_G = -\mathbb{E}_{z \sim p_z(z)}[D(G(z))]
% \]

% Here, the generator seeks to minimize the critic's score on generated data, pushing it closer to the real data distribution.

% ### Lipschitz Continuity and Weight Clipping

% One key requirement in WGAN is enforcing Lipschitz continuity on the critic to ensure the Wasserstein distance is meaningful. This is typically done by constraining the critic's weights through weight clipping, meaning the weights of the critic are forced to remain within a certain range \( [-c, c] \), where \( c \) is a small constant (e.g., 0.01).

% Mathematically, the weight clipping operation is represented as:

% \[
% w \leftarrow \text{clip}(w, -c, c)
% \]

% While weight clipping is simple to implement, later improvements such as WGAN-GP (WGAN with Gradient Penalty) replaced clipping with a more sophisticated gradient penalty technique to ensure Lipschitz continuity while improving performance and avoiding issues caused by clipping.

% WGANs have significantly improved the training of GANs by providing more stable gradients, better convergence, and a meaningful loss metric that correlates with the quality of generated data  .

% ---

% **References:**
% 1. Arjovsky, M., Chintala, S., & Bottou, L. (2017). *Wasserstein GAN*. arXiv preprint arXiv:1701.07875.
% 2. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C. (2017). *Improved Training of Wasserstein GANs*.