\subsection{Programming setup}
Data augmentation using deep learing is a highly complicated process, so choosing the right tooling plays a crucial role in it. Lack of knowledge about modern technologies that help with programming environment setup may lead to many hours of additional time spent on the project that are not specifically related to the core analysis. In order to reduce the time spent on configuration errors, modern state-of-the-art technologies were employed to mitigate this issue.  

\input{detailed_engineering/programming_setup}
\newpage
\subsection{Hardware}
Training neural networks demands substantial computational resources, particularly for the latest architecture models. These models typically consume significant memory, making it impractical to train them on personal computers. To address this, two Nvidia Quadro RTX 8000 GPUs, each equipped with 48GB of RAM, were utilized. 

\subsection{Hardware utilisation - Multiple GPU utilisation techniques}
Training on a single GPU is straightforward. However, when a single card falls short, the challenge of leveraging multiple GPUs arises. The solution varies based on the underlying reason for employing multiple GPUs.
\paragraph{Distributed data parallel}\mbox{} \\
If a single GPU can handle training a neural network but you wish to speed up the process with an additional GPU, the Distributed Data Parallel (DDP) technique can be used. This approach distributes the neural network model and data across both GPUs, allowing for concurrent training of two model instances. However, during the backpropagation phase, the gradients are synchronized and averaged.

This technique was used to train most of the models presented later, since it sped up the training twice. 

\paragraph{Distributed data parallel sharded}\mbox{} \\
If one GPU is unable to fit into its memory model, then the distributed data parallel shared technique can be used to solve the issue. Splits the model layers between cards and then uses cross-device communication techniques to train the model. However, this solution slows down the training due to the overhead of cross-device communication.

\newpage 
\subsection{Training monitoring}
Training of neural networks often takes a lot of time - hours, days, weeks and sometime even months. It is crucial to monitor it in order not to waste resources on unsuccessful training. Whats more - when there are many models that are trained or there are many attempts of training it is easy to loose track of what has been accomplished, which models were already trained and what were the results. 
Monitoring enables the researcher to have visibility of important metrics for the model. Thus, unsuccessful training may be stopped ahead of time. It also helps to visualize training progress, model output and makes it possible to share the results with other researchers during the training. 
\paragraph{Tensorboard}
Tensorboard is an open source project that enables user to locally monitor training of a neural networks. It makes it possible to:
- gather metrics,
- visualize model outputs,
- organize model trainings in the friendly user interface.

\paragraph{Wandb}
Wandb is a commercial product that allows to monitor training of Machine Learning models, especially deep learning ones. W sk≈Çad monitoringu wchodzi:
- gathering statistics and metrics
- comparison of statistics,
- hardware monitoring,
- configuration monitoring. 
- reports of training. 

It can be synchronised with Tensorobard. The metrics logged there can be viewed in the WanDB website. They give researchers from the academia free resources to use. 

Thanks to the tool it was possible to monitor whether model converges or not.

\newpage
\subsection{Dataset}
\paragraph{Data source}
\paragraph{Data format - DICOM}
- NII.GZ
- 
\paragraph{Data characteristics}
- count
- value range
- number of layers
\paragraph{Data transformations}

\newpage
\subsection{Chosen approaches}
% \subsubsection{WGAN}
% \subsubsection{Autoencoder}
\subsubsection{VAE + LDM}
\input{}
\input{detailed_engineering/monai_ddpm}
\newpage
\subsubsection{Transformers CT: VQVAE + Transformers}
\input{detailed_engineering/german_vqvae}
\newpage
\subsubsection{Medical Diffusion: VQGAN + LDM}
\input{detailed_engineering/medical_diffusion}

\subsection{Evaluation}

In the table below, autoencoders with their reconstruction quality metrics are presented. 
In addition, the $LDM$ generation quality was measured in the same way. The generative model is treated as if it were a reconstructor, the same as an autoencoder.

In order to profesionally evaluate quality of the generated dataset, the FID score should be calculated. Since the FID score is related to images, not scans or videos, each layer has been analyzed separately. 
A synthetic data set of 28 images (length of the original data set) was created. Then each corresponding layer of scans was analyzed by FID. The mean FID score for each layer in the synthethic dataset is equal to 244.65. 

The number is high compared to the ones in the VQGAN paper\cite{esser2021tamingtransformershighresolutionimage}. It could be caused by the fact that LDM may not generate the internals of the body in the correct order, as can be seen in the figure \ref{fig:ldm-success-comparison}. However, it should be acknowledged by a radiology specialist or doctor of medicine.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Metric} & VAE & VQVAE1 & Medical Diffusion VQVAE & \textbf{LDM} \\
\hline
$\overline{L1}$ & 0.0309 & 0.0212 & 0.0144 & 0.0845 \\
\hline
$\overline{L2}$ & 0.0043 & 0.0023 & 0.0007 & 0.0270 \\
\hline
$\overline{SSIM}$ & 0.8690 & 0.9399 & 0.9795 & 0.5954 \\
\hline
$\overline{LPIPS}$ & 2.2485 & 1.3820 & 0.8898 & 3.3926 \\
\hline
\end{tabular}
\caption{Mean metrics of the model on the whole dataset as inputs and their reconstructions.}
\label{table:metrics}
\end{table}


% \input{monai}